# Default values for victoria-metrics-alert.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  # -- Image pull secrets, that can be shared across multiple helm charts
  imagePullSecrets: []
  image:
    # -- Image registry, that can be shared across multiple helm charts
    registry: ''
  # -- Openshift security context compatibility configuration
  compatibility:
    openshift:
      adaptSecurityContext: 'auto'
  cluster:
    # -- K8s cluster domain suffix, uses for building storage pods' FQDN. Details are [here](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/)
    dnsDomain: cluster.local.

serviceAccount:
  # -- Specifies whether a service account should be created
  create: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:
  # -- Mount API token to pod directly
  automountToken: true

# -- Override chart name
nameOverride: ''

server:
  # -- Override default `app` label name
  name: ''
  # -- Create vmalert component
  enabled: true
  # -- VMAlert image configuration
  image:
    registry: ''
    repository: victoriametrics/vmalert
    tag: '' # rewrites Chart.AppVersion
    # Variant of the image to use.
    # e.g. enterprise, scratch
    variant: ''
    pullPolicy: IfNotPresent
  # -- Override vmalert resources fullname
  fullnameOverride: ''
  # -- Image pull secrets
  imagePullSecrets: []

  # -- See `kubectl explain poddisruptionbudget.spec` for more. Or check [docs](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)
  podDisruptionBudget:
    enabled: false
    # minAvailable: 1
    # maxUnavailable: 1
    labels: {}

  # -- Additional environment variables (ex.: secret tokens, flags). Check [here](https://docs.victoriametrics.com/#environment-variables) for details.
  env:
    []
    # - name: VM_remoteWrite_basicAuth_password
    #   valueFrom:
    #     secretKeyRef:
    #       name: auth_secret
    #       key: password

  # -- Specify alternative source for env variables
  envFrom:
    []
    #- configMapRef:
    #    name: special-config

  probe:
    # -- Readiness probe
    readiness:
      httpGet: {}
      initialDelaySeconds: 5
      periodSeconds: 15
      timeoutSeconds: 5
      failureThreshold: 3
    # -- Liveness probe
    liveness:
      tcpSocket: {}
      initialDelaySeconds: 5
      periodSeconds: 15
      timeoutSeconds: 5
      failureThreshold: 3
    # -- Startup probe
    startup: {}

  # -- Replica count
  replicaCount: 1

  # -- Deployment strategy, set to standard k8s default
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%

  # -- Specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing/terminating
  # 0 is the standard k8s default
  minReadySeconds: 0

  # -- VMAlert reads metrics from source, next section represents its configuration. It can be any service which supports
  # MetricsQL or PromQL.
  datasource:
    url: 'http://vm-metrics.cloudydad.world'
    # -- Basic auth for datasource
    basicAuth:
      username: ''
      password: ''
      # -- Auth based on Bearer token for datasource
    bearer:
      # -- Token with Bearer token. You can use one of token or tokenFile. You don't need to add "Bearer" prefix string
      token: ''
      # -- Token Auth file with Bearer token. You can use one of token or tokenFile
      tokenFile: ''

  remote:
    write:
      # -- VMAlert remote write URL
      url: 'http://vm-metrics.cloudydad.world'
      # -- Basic auth for remote write
      basicAuth:
        username: ''
        password: ''
      # -- Auth based on Bearer token for remote write
      bearer:
        # -- Token with Bearer token. You can use one of token or tokenFile. You don't need to add "Bearer" prefix string
        token: ''
        # -- Token Auth file with Bearer token. You can use one of token or tokenFile
        tokenFile: ''
    read:
      # -- VMAlert remote read URL
      url: 'http://vm-metrics.cloudydad.world'
      # -- Basic auth for remote read
      basicAuth:
        username: ''
        password: ''
      # -- Auth based on Bearer token for remote read
      bearer:
        # -- Token with Bearer token. You can use one of token or tokenFile. You don't need to add "Bearer" prefix string
        token: ''
        # -- Token Auth file with Bearer token. You can use one of token or tokenFile
        tokenFile: ''

  # -- Notifier to use for alerts.
  # Multiple notifiers can be enabled by using `notifiers` section
  notifier:
    alertmanager:
      url: 'http://alertmanager.cloudydad.world'
      # -- Basic auth for alertmanager
      basicAuth:
        username: ''
        password: ''
        # -- Auth based on Bearer token for alertmanager
      bearer:
        # -- Token with Bearer token. You can use one of token or tokenFile. You don't need to add "Bearer" prefix string
        token: ''
        # -- Token Auth file with Bearer token. You can use one of token or tokenFile
        tokenFile: ''

  # -- Additional notifiers to use for alerts
  notifiers:
    []
    # - alertmanager:
    #    url: ""
    #    basicAuth:
    #      username: ""
    #      password: ""
    #    bearer:
    #      token: ""
    #      tokenFile: ""

  # -- Extra command line arguments for container of component
  extraArgs:
    envflag.enable: 'true'
    envflag.prefix: VM_
    loggerFormat: json
    rule:
      - /config/alert-rules.yaml

  # -- Additional hostPath mounts
  extraHostPathMounts:
    []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   subPath: ""
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  # -- Extra Volumes for the pod
  extraVolumes:
    []
    # - name: example
    #   configMap:
    #     name: example

  # -- Extra Volume Mounts for the container.
  # Expects a lice of [volume mounts](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#volumemount-v1-core)
  extraVolumeMounts:
    []
    # - name: example
    #   mountPath: /example
    #   subPath: ""

  # -- Additional containers to run in the same pod
  extraContainers:
    []
    #- name: config-reloader
    #  image: reloader-image

  service:
    # -- Service annotations
    annotations: {}
    # -- Service labels
    labels: {}
    # -- Service ClusterIP
    clusterIP: ''
    # -- Service external IPs. Check [here](https://kubernetes.io/docs/user-guide/services/#external-ips) for details
    externalIPs: []
    # -- Service load balacner IP
    loadBalancerIP: ''
    # -- Load balancer source range
    loadBalancerSourceRanges: []
    # -- Service port
    servicePort: 8880
    # nodePort: 30000
    # -- Service type
    type: ClusterIP
    # -- Service external traffic policy. Check [here](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip) for details
    externalTrafficPolicy: ''
    # -- Health check node port for a service. Check [here](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip) for details
    healthCheckNodePort: ''
    # -- Service IP family policy. Check [here](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services) for details.
    ipFamilyPolicy: ''
    # -- List of service IP families. Check [here](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services) for details.
    ipFamilies: []

  ingress:
    # -- Enable deployment of ingress for vmalert component
    enabled: true

    # -- Ingress annotations
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    # -- Ingress extra labels
    extraLabels: {}

    # -- Array of host objects
    hosts:
      - name: vm-alert.cloudydad.world
        path:
          - /
        port: http

    # -- Array of TLS objects
    tls: []
    #   - secretName: vmselect-ingress-tls
    #     hosts:
    #       - vmselect.local

    # -- Ingress controller class name
    ingressClassName: 'traefik'

    # -- Ingress path type
    pathType: Prefix

  # -- Pod's security context. Details are [here](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
  podSecurityContext:
    enabled: true
  # fsGroup: 2000

  # -- Security context to be added to server pods
  securityContext:
    enabled: true
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
  # runAsUser: 1000

  # -- Resource object. Details are [here](http://kubernetes.io/docs/user-guide/compute-resources/)
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
  #   memory: 128Mi

  # -- Annotations to be added to the deployment
  annotations: {}
  # -- Labels to be added to the deployment
  labels: {}

  # -- Annotations to be added to pod
  podAnnotations: {}

  # -- Pod's additional labels
  podLabels: {}

  # -- Pod's node selector. Details are [here](https://kubernetes.io/docs/user-guide/node-selection/)
  nodeSelector: {}

  # -- Name of Priority Class
  priorityClassName: ''

  # -- Node tolerations for server scheduling to nodes with taints. Details are [here](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
  tolerations: []

  # -- Pod affinity
  affinity: {}

  # -- VMAlert alert rules configuration configuration.
  # Use existing configmap if specified
  configMap: ''
  # -- VMAlert configuration
  config:
    alerts:
      groups:
        - name: alertmanager.rules
          rules:
            - alert: AlertmanagerFailedReload
              annotations:
                description: Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload
                summary: Reloading an Alertmanager configuration has failed.
              expr: |
                # Without max_over_time, failed scrapes could create false negatives, see
                # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
                max_over_time(alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"}[5m]) == 0
              for: 10m
              labels:
                severity: critical
            - alert: AlertmanagerMembersInconsistent
              annotations:
                description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent
                summary: A member of an Alertmanager cluster has not found all other cluster members.
              expr: |
                # Without max_over_time, failed scrapes could create false negatives, see
                # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
                  max_over_time(alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}[5m])
                < on (namespace,service) group_left
                  count by (namespace,service) (max_over_time(alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}[5m]))
              for: 15m
              labels:
                severity: critical
            - alert: AlertmanagerFailedToSendAlerts
              annotations:
                description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts
                summary: An Alertmanager instance failed to send notifications.
              expr: |
                (
                  rate(alertmanager_notifications_failed_total{job="alertmanager-main",namespace="monitoring"}[5m])
                /
                  ignoring (reason) group_left rate(alertmanager_notifications_total{job="alertmanager-main",namespace="monitoring"}[5m])
                )
                > 0.01
              for: 5m
              labels:
                severity: warning
            - alert: AlertmanagerClusterFailedToSendAlerts
              annotations:
                description: The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
                summary: All Alertmanager instances in a cluster failed to send notifications to a critical integration.
              expr: |
                min by (namespace,service, integration) (
                  rate(alertmanager_notifications_failed_total{job="alertmanager-main",namespace="monitoring", integration=~`.*`}[5m])
                /
                  ignoring (reason) group_left rate(alertmanager_notifications_total{job="alertmanager-main",namespace="monitoring", integration=~`.*`}[5m])
                )
                > 0.01
              for: 5m
              labels:
                severity: critical
            - alert: AlertmanagerClusterFailedToSendAlerts
              annotations:
                description: The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
                summary: All Alertmanager instances in a cluster failed to send notifications to a non-critical integration.
              expr: |
                min by (namespace,service, integration) (
                  rate(alertmanager_notifications_failed_total{job="alertmanager-main",namespace="monitoring", integration!~`.*`}[5m])
                /
                  ignoring (reason) group_left rate(alertmanager_notifications_total{job="alertmanager-main",namespace="monitoring", integration!~`.*`}[5m])
                )
                > 0.01
              for: 5m
              labels:
                severity: warning
            - alert: AlertmanagerConfigInconsistent
              annotations:
                description: Alertmanager instances within the {{$labels.job}} cluster have different configurations.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent
                summary: Alertmanager instances within the same cluster have different configurations.
              expr: |
                count by (namespace,service) (
                  count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="monitoring"})
                )
                != 1
              for: 20m
              labels:
                severity: critical
            - alert: AlertmanagerClusterDown
              annotations:
                description: '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown
                summary: Half or more of the Alertmanager instances within the same cluster are down.
              expr: |
                (
                  count by (namespace,service) (
                    avg_over_time(up{job="alertmanager-main",namespace="monitoring"}[5m]) < 0.5
                  )
                /
                  count by (namespace,service) (
                    up{job="alertmanager-main",namespace="monitoring"}
                  )
                )
                >= 0.5
              for: 5m
              labels:
                severity: critical
            - alert: AlertmanagerClusterCrashlooping
              annotations:
                description: '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping
                summary: Half or more of the Alertmanager instances within the same cluster are crashlooping.
              expr: |
                (
                  count by (namespace,service) (
                    changes(process_start_time_seconds{job="alertmanager-main",namespace="monitoring"}[10m]) > 4
                  )
                /
                  count by (namespace,service) (
                    up{job="alertmanager-main",namespace="monitoring"}
                  )
                )
                >= 0.5
              for: 5m
              labels:
                severity: critical
        - name: GrafanaAlerts
          rules:
            - alert: GrafanaRequestsFailing
              annotations:
                message: '{{ $labels.namespace }}/{{ $labels.job }}/{{ $labels.handler }} is experiencing {{ $value | humanize }}% errors'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/grafana/grafanarequestsfailing
              expr: |
                100 * namespace_job_handler_statuscode:grafana_http_request_duration_seconds_count:rate5m{handler!~"/api/datasources/proxy/:id.*|/api/ds/query|/api/tsdb/query", status_code=~"5.."}
                / ignoring (status_code)
                sum without (status_code) (namespace_job_handler_statuscode:grafana_http_request_duration_seconds_count:rate5m{handler!~"/api/datasources/proxy/:id.*|/api/ds/query|/api/tsdb/query"})
                > 50
              for: 5m
              labels:
                severity: warning
        - name: grafana_rules
          rules:
            - expr: |
                sum by (namespace, job, handler, status_code) (rate(grafana_http_request_duration_seconds_count[5m]))
              record: namespace_job_handler_statuscode:grafana_http_request_duration_seconds_count:rate5m
        - name: kube-state-metrics
          rules:
            - alert: KubeStateMetricsListErrors
              annotations:
                description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors
                summary: kube-state-metrics is experiencing errors in list operations.
              expr: |
                (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
                  /
                sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
                > 0.01
              for: 15m
              labels:
                severity: critical
            - alert: KubeStateMetricsWatchErrors
              annotations:
                description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors
                summary: kube-state-metrics is experiencing errors in watch operations.
              expr: |
                (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
                  /
                sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
                > 0.01
              for: 15m
              labels:
                severity: critical
            - alert: KubeStateMetricsShardingMismatch
              annotations:
                description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
                summary: kube-state-metrics sharding is misconfigured.
              expr: |
                stdvar (kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) != 0
              for: 15m
              labels:
                severity: critical
            - alert: KubeStateMetricsShardsMissing
              annotations:
                description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing
                summary: kube-state-metrics shards are missing.
              expr: |
                2^max(kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) - 1
                  -
                sum( 2 ^ max by (cluster, shard_ordinal) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"}) ) by (cluster)
                != 0
              for: 15m
              labels:
                severity: critical
        - name: kubernetes-apps
          rules:
            - alert: KubePodCrashLooping
              annotations:
                description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
                summary: Pod is crash looping.
              expr: |
                max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", job="kube-state-metrics"}[5m]) >= 1
              for: 15m
              labels:
                severity: warning
            - alert: KubePodNotReady
              annotations:
                description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
                summary: Pod has been in a non-ready state for more than 15 minutes.
              expr: |
                sum by (namespace, pod, cluster) (
                  max by(namespace, pod, cluster) (
                    kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown|Failed"}
                  ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
                    1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
                  )
                ) > 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeDeploymentGenerationMismatch
              annotations:
                description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
                summary: Deployment generation mismatch due to possible roll-back
              expr: |
                kube_deployment_status_observed_generation{job="kube-state-metrics"}
                  !=
                kube_deployment_metadata_generation{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: warning
            - alert: KubeDeploymentReplicasMismatch
              annotations:
                description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
                summary: Deployment has not matched the expected number of replicas.
              expr: |
                (
                  kube_deployment_spec_replicas{job="kube-state-metrics"}
                    >
                  kube_deployment_status_replicas_available{job="kube-state-metrics"}
                ) and (
                  changes(kube_deployment_status_replicas_updated{job="kube-state-metrics"}[10m])
                    ==
                  0
                )
              for: 15m
              labels:
                severity: warning
            - alert: KubeDeploymentRolloutStuck
              annotations:
                description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck
                summary: Deployment rollout is not progressing.
              expr: |
                kube_deployment_status_condition{condition="Progressing", status="false",job="kube-state-metrics"}
                != 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeStatefulSetReplicasMismatch
              annotations:
                description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
                summary: StatefulSet has not matched the expected number of replicas.
              expr: |
                (
                  kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
                    !=
                  kube_statefulset_status_replicas{job="kube-state-metrics"}
                ) and (
                  changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[10m])
                    ==
                  0
                )
              for: 15m
              labels:
                severity: warning
            - alert: KubeStatefulSetGenerationMismatch
              annotations:
                description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
                summary: StatefulSet generation mismatch due to possible roll-back
              expr: |
                kube_statefulset_status_observed_generation{job="kube-state-metrics"}
                  !=
                kube_statefulset_metadata_generation{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: warning
            - alert: KubeStatefulSetUpdateNotRolledOut
              annotations:
                description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
                summary: StatefulSet update has not been rolled out.
              expr: |
                (
                  max by(namespace, statefulset, job, cluster) (
                    kube_statefulset_status_current_revision{job="kube-state-metrics"}
                      unless
                    kube_statefulset_status_update_revision{job="kube-state-metrics"}
                  )
                    *
                  (
                    kube_statefulset_replicas{job="kube-state-metrics"}
                      !=
                    kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
                  )
                )  and (
                  changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[5m])
                    ==
                  0
                )
              for: 15m
              labels:
                severity: warning
            - alert: KubeDaemonSetRolloutStuck
              annotations:
                description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
                summary: DaemonSet rollout is stuck.
              expr: |
                (
                  (
                    kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"}
                    !=
                    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                  ) or (
                    kube_daemonset_status_number_misscheduled{job="kube-state-metrics"}
                    !=
                    0
                  ) or (
                    kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics"}
                    !=
                    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                  ) or (
                    kube_daemonset_status_number_available{job="kube-state-metrics"}
                    !=
                    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                  )
                ) and (
                  changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics"}[5m])
                    ==
                  0
                )
              for: 15m
              labels:
                severity: warning
            - alert: KubeContainerWaiting
              annotations:
                description: 'pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour. (reason: "{{ $labels.reason }}").'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
                summary: Pod container waiting longer than 1 hour
              expr: |
                kube_pod_container_status_waiting_reason{reason!="CrashLoopBackOff", job="kube-state-metrics"} > 0
              for: 1h
              labels:
                severity: warning
            - alert: KubeDaemonSetNotScheduled
              annotations:
                description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
                summary: DaemonSet pods are not scheduled.
              expr: |
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                  -
                kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
              for: 10m
              labels:
                severity: warning
            - alert: KubeDaemonSetMisScheduled
              annotations:
                description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
                summary: DaemonSet pods are misscheduled.
              expr: |
                kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeJobNotCompleted
              annotations:
                description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted
                summary: Job did not complete in time
              expr: |
                time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{job="kube-state-metrics"}
                  and
                kube_job_status_active{job="kube-state-metrics"} > 0) > 43200
              labels:
                severity: warning
            - alert: KubeJobFailed
              annotations:
                description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
                summary: Job failed to complete.
              expr: |
                kube_job_failed{job="kube-state-metrics"}  > 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeHpaReplicasMismatch
              annotations:
                description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch
                summary: HPA has not matched desired number of replicas.
              expr: |
                (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics"}
                  !=
                kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"})
                  and
                (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
                  >
                kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics"})
                  and
                (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
                  <
                kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"})
                  and
                changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}[15m]) == 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeHpaMaxedOut
              annotations:
                description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
                summary: HPA is running at max replicas
              expr: |
                kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
                  ==
                kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: warning
        - name: kubernetes-resources
          rules:
            - alert: KubeCPUOvercommit
              annotations:
                description: Cluster {{ $labels.cluster }} has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
                summary: Cluster has overcommitted CPU resource requests.
              expr: |
                sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
                and
                (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
              for: 10m
              labels:
                severity: warning
            - alert: KubeMemoryOvercommit
              annotations:
                description: Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
                summary: Cluster has overcommitted memory resource requests.
              expr: |
                sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
                and
                (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
              for: 10m
              labels:
                severity: warning
            - alert: KubeCPUQuotaOvercommit
              annotations:
                description: Cluster {{ $labels.cluster }}  has overcommitted CPU resource requests for Namespaces.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit
                summary: Cluster has overcommitted CPU resource requests.
              expr: |
                sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"})) by (cluster)
                  /
                sum(kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"}) by (cluster)
                  > 1.5
              for: 5m
              labels:
                severity: warning
            - alert: KubeMemoryQuotaOvercommit
              annotations:
                description: Cluster {{ $labels.cluster }}  has overcommitted memory resource requests for Namespaces.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit
                summary: Cluster has overcommitted memory resource requests.
              expr: |
                sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"})) by (cluster)
                  /
                sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)
                  > 1.5
              for: 5m
              labels:
                severity: warning
            - alert: KubeQuotaAlmostFull
              annotations:
                description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
                summary: Namespace quota is going to be full.
              expr: |
                kube_resourcequota{job="kube-state-metrics", type="used"}
                  / ignoring(instance, job, type)
                (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                  > 0.9 < 1
              for: 15m
              labels:
                severity: info
            - alert: KubeQuotaFullyUsed
              annotations:
                description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused
                summary: Namespace quota is fully used.
              expr: |
                kube_resourcequota{job="kube-state-metrics", type="used"}
                  / ignoring(instance, job, type)
                (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                  == 1
              for: 15m
              labels:
                severity: info
            - alert: KubeQuotaExceeded
              annotations:
                description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded
                summary: Namespace quota has exceeded the limits.
              expr: |
                kube_resourcequota{job="kube-state-metrics", type="used"}
                  / ignoring(instance, job, type)
                (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                  > 1
              for: 15m
              labels:
                severity: warning
            - alert: CPUThrottlingHigh
              annotations:
                description: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh
                summary: Processes experience elevated CPU throttling.
              expr: |
                sum(increase(container_cpu_cfs_throttled_periods_total{container!="", job="kubelet", metrics_path="/metrics/cadvisor", }[5m])) without (id, metrics_path, name, image, endpoint, job, node)
                  /
                sum(increase(container_cpu_cfs_periods_total{job="kubelet", metrics_path="/metrics/cadvisor", }[5m])) without (id, metrics_path, name, image, endpoint, job, node)
                  > ( 25 / 100 )
              for: 15m
              labels:
                severity: info
        - name: kubernetes-storage
          rules:
            - alert: KubePersistentVolumeFillingUp
              annotations:
                description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
                summary: PersistentVolume is filling up.
              expr: |
                (
                  kubelet_volume_stats_available_bytes{job="kubelet", metrics_path="/metrics"}
                    /
                  kubelet_volume_stats_capacity_bytes{job="kubelet", metrics_path="/metrics"}
                ) < 0.03
                and
                kubelet_volume_stats_used_bytes{job="kubelet", metrics_path="/metrics"} > 0
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
              for: 1m
              labels:
                severity: critical
            - alert: KubePersistentVolumeFillingUp
              annotations:
                description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
                summary: PersistentVolume is filling up.
              expr: |
                (
                  kubelet_volume_stats_available_bytes{job="kubelet", metrics_path="/metrics"}
                    /
                  kubelet_volume_stats_capacity_bytes{job="kubelet", metrics_path="/metrics"}
                ) < 0.15
                and
                kubelet_volume_stats_used_bytes{job="kubelet", metrics_path="/metrics"} > 0
                and
                predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
              for: 1h
              labels:
                severity: warning
            - alert: KubePersistentVolumeInodesFillingUp
              annotations:
                description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
                summary: PersistentVolumeInodes are filling up.
              expr: |
                (
                  kubelet_volume_stats_inodes_free{job="kubelet", metrics_path="/metrics"}
                    /
                  kubelet_volume_stats_inodes{job="kubelet", metrics_path="/metrics"}
                ) < 0.03
                and
                kubelet_volume_stats_inodes_used{job="kubelet", metrics_path="/metrics"} > 0
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
              for: 1m
              labels:
                severity: critical
            - alert: KubePersistentVolumeInodesFillingUp
              annotations:
                description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
                summary: PersistentVolumeInodes are filling up.
              expr: |
                (
                  kubelet_volume_stats_inodes_free{job="kubelet", metrics_path="/metrics"}
                    /
                  kubelet_volume_stats_inodes{job="kubelet", metrics_path="/metrics"}
                ) < 0.15
                and
                kubelet_volume_stats_inodes_used{job="kubelet", metrics_path="/metrics"} > 0
                and
                predict_linear(kubelet_volume_stats_inodes_free{job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                unless on(cluster, namespace, persistentvolumeclaim)
                kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
              for: 1h
              labels:
                severity: warning
            - alert: KubePersistentVolumeErrors
              annotations:
                description: The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
                summary: PersistentVolume is having issues with provisioning.
              expr: |
                kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
              for: 5m
              labels:
                severity: critical
        - name: kubernetes-system
          rules:
            - alert: KubeVersionMismatch
              annotations:
                description: There are {{ $value }} different semantic versions of Kubernetes components running.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch
                summary: Different semantic versions of Kubernetes components running.
              expr: |
                count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*"))) > 1
              for: 15m
              labels:
                severity: warning
            - alert: KubeClientErrors
              annotations:
                description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors
                summary: Kubernetes API server client is experiencing errors.
              expr: |
                (sum(rate(rest_client_requests_total{job="apiserver",code=~"5.."}[5m])) by (cluster, instance, job, namespace)
                  /
                sum(rate(rest_client_requests_total{job="apiserver"}[5m])) by (cluster, instance, job, namespace))
                > 0.01
              for: 15m
              labels:
                severity: warning
        - name: kube-apiserver-slos
          rules:
            - alert: KubeAPIErrorBudgetBurn
              annotations:
                description: The API server is burning too much error budget.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
                summary: The API server is burning too much error budget.
              expr: |
                sum by(cluster) (apiserver_request:burnrate1h) > (14.40 * 0.01000)
                and on(cluster)
                sum by(cluster) (apiserver_request:burnrate5m) > (14.40 * 0.01000)
              for: 2m
              labels:
                long: 1h
                severity: critical
                short: 5m
            - alert: KubeAPIErrorBudgetBurn
              annotations:
                description: The API server is burning too much error budget.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
                summary: The API server is burning too much error budget.
              expr: |
                sum by(cluster) (apiserver_request:burnrate6h) > (6.00 * 0.01000)
                and on(cluster)
                sum by(cluster) (apiserver_request:burnrate30m) > (6.00 * 0.01000)
              for: 15m
              labels:
                long: 6h
                severity: critical
                short: 30m
            - alert: KubeAPIErrorBudgetBurn
              annotations:
                description: The API server is burning too much error budget.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
                summary: The API server is burning too much error budget.
              expr: |
                sum by(cluster) (apiserver_request:burnrate1d) > (3.00 * 0.01000)
                and on(cluster)
                sum by(cluster) (apiserver_request:burnrate2h) > (3.00 * 0.01000)
              for: 1h
              labels:
                long: 1d
                severity: warning
                short: 2h
            - alert: KubeAPIErrorBudgetBurn
              annotations:
                description: The API server is burning too much error budget.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
                summary: The API server is burning too much error budget.
              expr: |
                sum by(cluster) (apiserver_request:burnrate3d) > (1.00 * 0.01000)
                and on(cluster)
                sum by(cluster) (apiserver_request:burnrate6h) > (1.00 * 0.01000)
              for: 3h
              labels:
                long: 3d
                severity: warning
                short: 6h
        - name: kubernetes-system-apiserver
          rules:
            - alert: KubeClientCertificateExpiration
              annotations:
                description: A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
                summary: Client certificate is about to expire.
              expr: |
                histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
                and
                on(job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0
              for: 5m
              labels:
                severity: warning
            - alert: KubeClientCertificateExpiration
              annotations:
                description: A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
                summary: Client certificate is about to expire.
              expr: |
                histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
                and
                on(job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0
              for: 5m
              labels:
                severity: critical
            - alert: KubeAggregatedAPIErrors
              annotations:
                description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors
                summary: Kubernetes aggregated API has reported errors.
              expr: |
                sum by(name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total{job="apiserver"}[10m])) > 4
              labels:
                severity: warning
            - alert: KubeAggregatedAPIDown
              annotations:
                description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown
                summary: Kubernetes aggregated API is down.
              expr: |
                (1 - max by(name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice{job="apiserver"}[10m]))) * 100 < 85
              for: 5m
              labels:
                severity: warning
            - alert: KubeAPIDown
              annotations:
                description: KubeAPI has disappeared from Prometheus target discovery.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown
                summary: Target disappeared from Prometheus target discovery.
              expr: |
                absent(up{job="apiserver"} == 1)
              for: 15m
              labels:
                severity: critical
            - alert: KubeAPITerminatedRequests
              annotations:
                description: The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests
                summary: The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
              expr: |
                sum by(cluster) (rate(apiserver_request_terminations_total{job="apiserver"}[10m])) / ( sum by(cluster) (rate(apiserver_request_total{job="apiserver"}[10m])) + sum by(cluster) (rate(apiserver_request_terminations_total{job="apiserver"}[10m])) ) > 0.20
              for: 5m
              labels:
                severity: warning
        - name: kubernetes-system-kubelet
          rules:
            - alert: KubeNodeNotReady
              annotations:
                description: '{{ $labels.node }} has been unready for more than 15 minutes.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
                summary: Node is not ready.
              expr: |
                kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeNodeUnreachable
              annotations:
                description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
                summary: Node is unreachable.
              expr: |
                (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
              for: 15m
              labels:
                severity: warning
            - alert: KubeletTooManyPods
              annotations:
                description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
                summary: Kubelet is running at capacity.
              expr: |
                count by(cluster, node) (
                  (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
                )
                /
                max by(cluster, node) (
                  kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
                ) > 0.95
              for: 15m
              labels:
                severity: info
            - alert: KubeNodeReadinessFlapping
              annotations:
                description: The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
                summary: Node readiness status is flapping.
              expr: |
                sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m])) by (cluster, node) > 2
              for: 15m
              labels:
                severity: warning
            - alert: KubeletPlegDurationHigh
              annotations:
                description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh
                summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
              expr: |
                node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
              for: 5m
              labels:
                severity: warning
            - alert: KubeletPodStartUpLatencyHigh
              annotations:
                description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh
                summary: Kubelet Pod startup latency is too high.
              expr: |
                histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
              for: 15m
              labels:
                severity: warning
            - alert: KubeletClientCertificateExpiration
              annotations:
                description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
                summary: Kubelet client certificate is about to expire.
              expr: |
                kubelet_certificate_manager_client_ttl_seconds < 604800
              labels:
                severity: warning
            - alert: KubeletClientCertificateExpiration
              annotations:
                description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
                summary: Kubelet client certificate is about to expire.
              expr: |
                kubelet_certificate_manager_client_ttl_seconds < 86400
              labels:
                severity: critical
            - alert: KubeletServerCertificateExpiration
              annotations:
                description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
                summary: Kubelet server certificate is about to expire.
              expr: |
                kubelet_certificate_manager_server_ttl_seconds < 604800
              labels:
                severity: warning
            - alert: KubeletServerCertificateExpiration
              annotations:
                description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
                summary: Kubelet server certificate is about to expire.
              expr: |
                kubelet_certificate_manager_server_ttl_seconds < 86400
              labels:
                severity: critical
            - alert: KubeletClientCertificateRenewalErrors
              annotations:
                description: Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
                summary: Kubelet has failed to renew its client certificate.
              expr: |
                increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeletServerCertificateRenewalErrors
              annotations:
                description: Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors
                summary: Kubelet has failed to renew its server certificate.
              expr: |
                increase(kubelet_server_expiration_renew_errors[5m]) > 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeletDown
              annotations:
                description: Kubelet has disappeared from Prometheus target discovery.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
                summary: Target disappeared from Prometheus target discovery.
              expr: |
                absent(up{job="kubelet", metrics_path="/metrics"} == 1)
              for: 15m
              labels:
                severity: critical
        - name: kubernetes-system-scheduler
          rules:
            - alert: KubeSchedulerDown
              annotations:
                description: KubeScheduler has disappeared from Prometheus target discovery.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown
                summary: Target disappeared from Prometheus target discovery.
              expr: |
                absent(up{job="kube-scheduler"} == 1)
              for: 15m
              labels:
                severity: critical
        - name: kubernetes-system-controller-manager
          rules:
            - alert: KubeControllerManagerDown
              annotations:
                description: KubeControllerManager has disappeared from Prometheus target discovery.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown
                summary: Target disappeared from Prometheus target discovery.
              expr: |
                absent(up{job="kube-controller-manager"} == 1)
              for: 15m
              labels:
                severity: critical
        - interval: 3m
          name: kube-apiserver-availability.rules
          rules:
            - expr: |
                avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 * 30
              record: code_verb:apiserver_request_total:increase30d
            - expr: |
                sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
              labels:
                verb: read
              record: code:apiserver_request_total:increase30d
            - expr: |
                sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
              labels:
                verb: write
              record: code:apiserver_request_total:increase30d
            - expr: |
                sum by (cluster, verb, scope, le) (increase(apiserver_request_sli_duration_seconds_bucket[1h]))
              record: cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h
            - expr: |
                sum by (cluster, verb, scope, le) (avg_over_time(cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h[30d]) * 24 * 30)
              record: cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d
            - expr: |
                sum by (cluster, verb, scope) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h{le="+Inf"})
              record: cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase1h
            - expr: |
                sum by (cluster, verb, scope) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le="+Inf"} * 24 * 30)
              record: cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d
            - expr: |
                1 - (
                  (
                    # write too slow
                    sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
                    -
                    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
                  ) +
                  (
                    # read too slow
                    sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~"LIST|GET"})
                    -
                    (
                      (
                        sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
                      +
                      sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
                    )
                  ) +
                  # errors
                  sum by (cluster) (code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
                )
                /
                sum by (cluster) (code:apiserver_request_total:increase30d)
              labels:
                verb: all
              record: apiserver_request:availability30d
            - expr: |
                1 - (
                  sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~"LIST|GET"})
                  -
                  (
                    # too slow
                    (
                      sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
                      or
                      vector(0)
                    )
                    +
                    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
                    +
                    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
                  )
                  +
                  # errors
                  sum by (cluster) (code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
                )
                /
                sum by (cluster) (code:apiserver_request_total:increase30d{verb="read"})
              labels:
                verb: read
              record: apiserver_request:availability30d
            - expr: |
                1 - (
                  (
                    # too slow
                    sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
                    -
                    sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
                  )
                  +
                  # errors
                  sum by (cluster) (code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
                )
                /
                sum by (cluster) (code:apiserver_request_total:increase30d{verb="write"})
              labels:
                verb: write
              record: apiserver_request:availability30d
            - expr: |
                sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
              labels:
                verb: read
              record: code_resource:apiserver_request_total:rate5m
            - expr: |
                sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
              labels:
                verb: write
              record: code_resource:apiserver_request_total:rate5m
            - expr: |
                sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"2.."}[1h]))
              record: code_verb:apiserver_request_total:increase1h
            - expr: |
                sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"3.."}[1h]))
              record: code_verb:apiserver_request_total:increase1h
            - expr: |
                sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"4.."}[1h]))
              record: code_verb:apiserver_request_total:increase1h
            - expr: |
                sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
              record: code_verb:apiserver_request_total:increase1h
        - name: kube-apiserver-burnrate.rules
          rules:
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[1d]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[1d]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[1d]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[1d]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
              labels:
                verb: read
              record: apiserver_request:burnrate1d
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[1h]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[1h]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[1h]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
              labels:
                verb: read
              record: apiserver_request:burnrate1h
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[2h]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[2h]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[2h]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[2h]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
              labels:
                verb: read
              record: apiserver_request:burnrate2h
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[30m]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[30m]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[30m]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
              labels:
                verb: read
              record: apiserver_request:burnrate30m
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[3d]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[3d]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[3d]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[3d]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
              labels:
                verb: read
              record: apiserver_request:burnrate3d
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[5m]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[5m]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[5m]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
              labels:
                verb: read
              record: apiserver_request:burnrate5m
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
                    -
                    (
                      (
                        sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[6h]))
                        or
                        vector(0)
                      )
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[6h]))
                      +
                      sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[6h]))
                    )
                  )
                  +
                  # errors
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
              labels:
                verb: read
              record: apiserver_request:burnrate6h
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1d]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1d]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
              labels:
                verb: write
              record: apiserver_request:burnrate1d
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1h]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
              labels:
                verb: write
              record: apiserver_request:burnrate1h
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[2h]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[2h]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
              labels:
                verb: write
              record: apiserver_request:burnrate2h
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[30m]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
              labels:
                verb: write
              record: apiserver_request:burnrate30m
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[3d]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[3d]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
              labels:
                verb: write
              record: apiserver_request:burnrate3d
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[5m]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
              labels:
                verb: write
              record: apiserver_request:burnrate5m
            - expr: |
                (
                  (
                    # too slow
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
                    -
                    sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[6h]))
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
                )
                /
                sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
              labels:
                verb: write
              record: apiserver_request:burnrate6h
        - name: kube-apiserver-histogram.rules
          rules:
            - expr: |
                histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))) > 0
              labels:
                quantile: '0.99'
                verb: read
              record: cluster_quantile:apiserver_request_sli_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_sli_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))) > 0
              labels:
                quantile: '0.99'
                verb: write
              record: cluster_quantile:apiserver_request_sli_duration_seconds:histogram_quantile
        - name: k8s.rules.container_cpu_usage_seconds_total
          rules:
            - expr: |
                sum by (cluster, namespace, pod, container) (
                  irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
                ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
                  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
        - name: k8s.rules.container_memory_working_set_bytes
          rules:
            - expr: |
                container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
                * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
                  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: node_namespace_pod_container:container_memory_working_set_bytes
        - name: k8s.rules.container_memory_rss
          rules:
            - expr: |
                container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
                * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
                  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: node_namespace_pod_container:container_memory_rss
        - name: k8s.rules.container_memory_cache
          rules:
            - expr: |
                container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
                * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
                  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: node_namespace_pod_container:container_memory_cache
        - name: k8s.rules.container_memory_swap
          rules:
            - expr: |
                container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
                * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
                  max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: node_namespace_pod_container:container_memory_swap
        - name: k8s.rules.container_memory_requests
          rules:
            - expr: |
                kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
                group_left() max by (namespace, pod, cluster) (
                  (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
                )
              record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
            - expr: |
                sum by (namespace, cluster) (
                    sum by (namespace, pod, cluster) (
                        max by (namespace, pod, container, cluster) (
                          kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                          kube_pod_status_phase{phase=~"Pending|Running"} == 1
                        )
                    )
                )
              record: namespace_memory:kube_pod_container_resource_requests:sum
        - name: k8s.rules.container_cpu_requests
          rules:
            - expr: |
                kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
                group_left() max by (namespace, pod, cluster) (
                  (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
                )
              record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
            - expr: |
                sum by (namespace, cluster) (
                    sum by (namespace, pod, cluster) (
                        max by (namespace, pod, container, cluster) (
                          kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                          kube_pod_status_phase{phase=~"Pending|Running"} == 1
                        )
                    )
                )
              record: namespace_cpu:kube_pod_container_resource_requests:sum
        - name: k8s.rules.container_memory_limits
          rules:
            - expr: |
                kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
                group_left() max by (namespace, pod, cluster) (
                  (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
                )
              record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
            - expr: |
                sum by (namespace, cluster) (
                    sum by (namespace, pod, cluster) (
                        max by (namespace, pod, container, cluster) (
                          kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                          kube_pod_status_phase{phase=~"Pending|Running"} == 1
                        )
                    )
                )
              record: namespace_memory:kube_pod_container_resource_limits:sum
        - name: k8s.rules.container_cpu_limits
          rules:
            - expr: |
                kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
                group_left() max by (namespace, pod, cluster) (
                (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
                )
              record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
            - expr: |
                sum by (namespace, cluster) (
                    sum by (namespace, pod, cluster) (
                        max by (namespace, pod, container, cluster) (
                          kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                          kube_pod_status_phase{phase=~"Pending|Running"} == 1
                        )
                    )
                )
              record: namespace_cpu:kube_pod_container_resource_limits:sum
        - name: k8s.rules.pod_owner
          rules:
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    label_replace(
                      kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                      "replicaset", "$1", "owner_name", "(.*)"
                    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                      1, max by (replicaset, namespace, owner_name) (
                        kube_replicaset_owner{job="kube-state-metrics"}
                      )
                    ),
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: deployment
              record: namespace_workload_pod:kube_pod_owner:relabel
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: daemonset
              record: namespace_workload_pod:kube_pod_owner:relabel
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: statefulset
              record: namespace_workload_pod:kube_pod_owner:relabel
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: job
              record: namespace_workload_pod:kube_pod_owner:relabel
        - name: kube-scheduler.rules
          rules:
            - expr: |
                histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.99'
              record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.99'
              record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.99'
              record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.9'
              record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.9'
              record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.9'
              record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.5'
              record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.5'
              record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
              labels:
                quantile: '0.5'
              record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - name: node.rules
          rules:
            - expr: |
                topk by(cluster, namespace, pod) (1,
                  max by (cluster, node, namespace, pod) (
                    label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
                ))
              record: 'node_namespace_pod:kube_pod_info:'
            - expr: |
                count by (cluster, node) (
                  node_cpu_seconds_total{mode="idle",job="node-exporter"}
                  * on (cluster, namespace, pod) group_left(node)
                  topk by(cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
                )
              record: node:node_num_cpu:sum
            - expr: |
                sum(
                  node_memory_MemAvailable_bytes{job="node-exporter"} or
                  (
                    node_memory_Buffers_bytes{job="node-exporter"} +
                    node_memory_Cached_bytes{job="node-exporter"} +
                    node_memory_MemFree_bytes{job="node-exporter"} +
                    node_memory_Slab_bytes{job="node-exporter"}
                  )
                ) by (cluster)
              record: :node_memory_MemAvailable_bytes:sum
            - expr: |
                avg by (cluster, node) (
                  sum without (mode) (
                    rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
                  )
                )
              record: node:node_cpu_utilization:ratio_rate5m
            - expr: |
                avg by (cluster) (
                  node:node_cpu_utilization:ratio_rate5m
                )
              record: cluster:node_cpu:ratio_rate5m
        - name: kubelet.rules
          rules:
            - expr: |
                histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
              labels:
                quantile: '0.99'
              record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
              labels:
                quantile: '0.9'
              record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
            - expr: |
                histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
              labels:
                quantile: '0.5'
              record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
        - name: node-exporter
          rules:
            - alert: NodeFilesystemSpaceFillingUp
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
                summary: Filesystem is predicted to run out of space within the next 24 hours.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 15
                and
                  predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemSpaceFillingUp
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
                summary: Filesystem is predicted to run out of space within the next 4 hours.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 10
                and
                  predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeFilesystemAlmostOutOfSpace
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
                summary: Filesystem has less than 5% space left.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 30m
              labels:
                severity: warning
            - alert: NodeFilesystemAlmostOutOfSpace
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
                summary: Filesystem has less than 3% space left.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 30m
              labels:
                severity: critical
            - alert: NodeFilesystemFilesFillingUp
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
                summary: Filesystem is predicted to run out of inodes within the next 24 hours.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 40
                and
                  predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemFilesFillingUp
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
                summary: Filesystem is predicted to run out of inodes within the next 4 hours.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 20
                and
                  predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeFilesystemAlmostOutOfFiles
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
                summary: Filesystem has less than 5% inodes left.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemAlmostOutOfFiles
              annotations:
                description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
                summary: Filesystem has less than 3% inodes left.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
                and
                  node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeNetworkReceiveErrs
              annotations:
                description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
                summary: Network interface is reporting many receive errors.
              expr: |
                rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
              for: 1h
              labels:
                severity: warning
            - alert: NodeNetworkTransmitErrs
              annotations:
                description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
                summary: Network interface is reporting many transmit errors.
              expr: |
                rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
              for: 1h
              labels:
                severity: warning
            - alert: NodeHighNumberConntrackEntriesUsed
              annotations:
                description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
                summary: Number of conntrack are getting close to the limit.
              expr: |
                (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
              labels:
                severity: warning
            - alert: NodeTextFileCollectorScrapeError
              annotations:
                description: Node Exporter text file collector on {{ $labels.instance }} failed to scrape.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
                summary: Node Exporter text file collector failed to scrape.
              expr: |
                node_textfile_scrape_error{job="node-exporter"} == 1
              labels:
                severity: warning
            - alert: NodeClockSkewDetected
              annotations:
                description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected
                summary: Clock skew detected.
              expr: |
                (
                  node_timex_offset_seconds{job="node-exporter"} > 0.05
                and
                  deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
                )
                or
                (
                  node_timex_offset_seconds{job="node-exporter"} < -0.05
                and
                  deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
                )
              for: 10m
              labels:
                severity: warning
            - alert: NodeClockNotSynchronising
              annotations:
                description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
                summary: Clock not synchronising.
              expr: |
                min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
                and
                node_timex_maxerror_seconds{job="node-exporter"} >= 16
              for: 10m
              labels:
                severity: warning
            - alert: NodeRAIDDegraded
              annotations:
                description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded
                summary: RAID Array is degraded.
              expr: |
                node_md_disks_required{job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"} - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}) > 0
              for: 15m
              labels:
                severity: critical
            - alert: NodeRAIDDiskFailure
              annotations:
                description: At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure
                summary: Failed device in RAID array.
              expr: |
                node_md_disks{state="failed",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"} > 0
              labels:
                severity: warning
            - alert: NodeFileDescriptorLimit
              annotations:
                description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
                summary: Kernel is predicted to exhaust file descriptors limit soon.
              expr: |
                (
                  node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
                )
              for: 15m
              labels:
                severity: warning
            - alert: NodeFileDescriptorLimit
              annotations:
                description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
                summary: Kernel is predicted to exhaust file descriptors limit soon.
              expr: |
                (
                  node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
                )
              for: 15m
              labels:
                severity: critical
            - alert: NodeCPUHighUsage
              annotations:
                description: |
                  CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage
                summary: High CPU usage.
              expr: |
                sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job="node-exporter", mode!="idle"}[2m]))) * 100 > 90
              for: 15m
              labels:
                severity: info
            - alert: NodeSystemSaturation
              annotations:
                description: |
                  System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
                  This might indicate this instance resources saturation and can cause it becoming unresponsive.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation
                summary: System saturated, load per core is very high.
              expr: |
                node_load1{job="node-exporter"}
                / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
              for: 15m
              labels:
                severity: warning
            - alert: NodeMemoryMajorPagesFaults
              annotations:
                description: |
                  Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
                  Please check that there is enough memory available at this instance.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults
                summary: Memory major page faults are occurring at very high rate.
              expr: |
                rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
              for: 15m
              labels:
                severity: warning
            - alert: NodeMemoryHighUtilization
              annotations:
                description: |
                  Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization
                summary: Host is running out of memory.
              expr: |
                100 - (node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"} * 100) > 90
              for: 15m
              labels:
                severity: warning
            - alert: NodeDiskIOSaturation
              annotations:
                description: |
                  Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf "%.2f" $value }}.
                  This symptom might indicate disk saturation.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation
                summary: Disk IO queue is high.
              expr: |
                rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m]) > 10
              for: 30m
              labels:
                severity: warning
            - alert: NodeSystemdServiceFailed
              annotations:
                description: Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed
                summary: Systemd service has entered failed state.
              expr: |
                node_systemd_unit_state{job="node-exporter", state="failed"} == 1
              for: 5m
              labels:
                severity: warning
            - alert: NodeBondingDegraded
              annotations:
                description: Bonding interface {{ $labels.master }} on {{ $labels.instance }} is in degraded state due to one or more slave failures.
                runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded
                summary: Bonding interface is degraded
              expr: |
                (node_bonding_slaves - node_bonding_active) != 0
              for: 5m
              labels:
                severity: warning
        - name: node-exporter.rules
          rules:
            - expr: |
                count without (cpu, mode) (
                  node_cpu_seconds_total{job="node-exporter",mode="idle"}
                )
              record: instance:node_num_cpu:sum
            - expr: |
                1 - avg without (cpu) (
                  sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[5m]))
                )
              record: instance:node_cpu_utilisation:rate5m
            - expr: |
                (
                  node_load1{job="node-exporter"}
                /
                  instance:node_num_cpu:sum{job="node-exporter"}
                )
              record: instance:node_load1_per_cpu:ratio
            - expr: |
                1 - (
                  (
                    node_memory_MemAvailable_bytes{job="node-exporter"}
                    or
                    (
                      node_memory_Buffers_bytes{job="node-exporter"}
                      +
                      node_memory_Cached_bytes{job="node-exporter"}
                      +
                      node_memory_MemFree_bytes{job="node-exporter"}
                      +
                      node_memory_Slab_bytes{job="node-exporter"}
                    )
                  )
                /
                  node_memory_MemTotal_bytes{job="node-exporter"}
                )
              record: instance:node_memory_utilisation:ratio
            - expr: |
                rate(node_vmstat_pgmajfault{job="node-exporter"}[5m])
              record: instance:node_vmstat_pgmajfault:rate5m
            - expr: |
                rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
              record: instance_device:node_disk_io_time_seconds:rate5m
            - expr: |
                rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
              record: instance_device:node_disk_io_time_weighted_seconds:rate5m
            - expr: |
                sum without (device) (
                  rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[5m])
                )
              record: instance:node_network_receive_bytes_excluding_lo:rate5m
            - expr: |
                sum without (device) (
                  rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[5m])
                )
              record: instance:node_network_transmit_bytes_excluding_lo:rate5m
            - expr: |
                sum without (device) (
                  rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[5m])
                )
              record: instance:node_network_receive_drop_excluding_lo:rate5m
            - expr: |
                sum without (device) (
                  rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[5m])
                )
              record: instance:node_network_transmit_drop_excluding_lo:rate5m

  # -- Vertical Pod Autoscaler
  verticalPodAutoscaler:
    # -- Use VPA for vmalert
    enabled: false
    # recommenders:
    #   - name: 'alternative'
    # updatePolicy:
    #   updateMode: "Auto"
    #   minReplicas: 1
    # resourcePolicy:
    #   containerPolicies:
    #     - containerName: '*'
    #       minAllowed:
    #         cpu: 100m
    #         memory: 128Mi
    #       maxAllowed:
    #         cpu: 1
    #         memory: 500Mi
    #       controlledResources: ["cpu", "memory"]

  # -- Additional initContainers to initialize the pod
  initContainers: []

serviceMonitor:
  # -- Enable deployment of Service Monitor for server component. This is Prometheus operator object
  enabled: true
  # -- Service Monitor labels
  extraLabels: {}
  # -- Service Monitor annotations
  annotations: {}
  # -- Service Monitor relabelings
  relabelings: []
  # -- Basic auth params for Service Monitor
  basicAuth: {}
  # -- Service Monitor metricRelabelings
  metricRelabelings: []
#    interval: 15s
#    scrapeTimeout: 5s
# -- Commented. HTTP scheme to use for scraping.
#    scheme: https
# -- Commented. TLS configuration to use when scraping the endpoint
#    tlsConfig:
#      insecureSkipVerify: true

alertmanager:
  # -- Create alertmanager resources
  enabled: true
  # -- Alertmanager Pod labels
  podLabels: {}
  # -- Override Alertmanager resources fullname
  fullnameOverride: ''
  # -- Alertmanager Pod annotations
  podAnnotations: {}
  # -- Alertmanager image configuration
  image:
    registry: ''
    repository: prom/alertmanager
    tag: v0.25.0
  # -- Alertmanager retention
  retention: 120h
  # -- Pod's node selector. Details are [here](https://kubernetes.io/docs/user-guide/node-selection/)
  nodeSelector: {}
  # -- Name of Priority Class
  priorityClassName: ''
  # -- Resource object. Details are [here](http://kubernetes.io/docs/user-guide/compute-resources/)
  resources: {}
  # -- Node tolerations for server scheduling to nodes with taints. Details are [here](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
  tolerations: []
  # -- Image pull secrets
  imagePullSecrets: []
  probe:
    # -- Readiness probe
    readiness:
      httpGet:
        path: '{{ ternary "" .baseURLPrefix (empty .baseURLPrefix) }}/-/ready'
        port: web
    # -- Liveness probe
    liveness:
      httpGet:
        path: '{{ ternary "" .baseURLPrefix (empty .baseURLPrefix) }}/-/healthy'
        port: web
    # -- Startup probe
    startup:
      httpGet:
        path: '{{ ternary "" .baseURLPrefix (empty .baseURLPrefix) }}/-/ready'
        port: web

  # -- Pod's security context. Details are [here](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
  podSecurityContext:
    enabled: false

  # -- Security context to be added to server pods
  securityContext:
    enabled: false
  # -- Alertmanager listen address
  listenAddress: '0.0.0.0:9093'
  # -- Extra command line arguments for container of component
  extraArgs: {}
  # -- Specify alternative source for env variables
  envFrom: []
  # -- External URL, that alertmanager will expose to receivers
  baseURL: ''
  # -- External URL Prefix, Prefix for the internal routes of web endpoints
  baseURLPrefix: ''
  # -- Use existing configmap if specified
  # otherwise .config values will be used
  configMap: ''
  # -- Alertmanager web configuration
  webConfig: {}
  # -- Alertmanager configuration
  config:
    global:
      resolve_timeout: 5m
    route:
      # default receiver
      receiver: devnull
      # tag to group by
      group_by: ['alertname']
      # How long to initially wait to send a notification for a group of alerts
      group_wait: 30s
      # How long to wait before sending a notification about new alerts that are added to a group
      group_interval: 10s
      # How long to wait before sending a notification again if it has already been sent successfully for an alert
      repeat_interval: 24h
    receivers:
      - name: devnull
  # -- Alertmanager extra templates
  templates: {}
  #  alertmanager.tmpl: |-
  service:
    # -- Service annotations
    annotations: {}
    # -- Service labels
    labels: {}
    # -- Service ClusterIP
    clusterIP: ''
    # -- Service external IPs. Check [here](https://kubernetes.io/docs/user-guide/services/#external-ips) for details
    externalIPs: []
    # -- Service load balacner IP
    loadBalancerIP: ''
    # -- Load balancer source range
    loadBalancerSourceRanges: []
    # -- Service port
    servicePort: 8880
    # nodePort: 30000
    # -- Service type
    type: ClusterIP
    # -- Service external traffic policy. Check [here](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip) for details
    externalTrafficPolicy: ''
    # -- Health check node port for a service. Check [here](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip) for details
    healthCheckNodePort: ''
    # -- Service IP family policy. Check [here](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services) for details.
    ipFamilyPolicy: ''
    # -- List of service IP families. Check [here](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services) for details.
    ipFamilies: []

  ingress:
    # -- Enable deployment of ingress for alertmanager component
    enabled: true

    # -- Ingress annotations
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    # -- Ingress extra labels
    extraLabels: {}

    # -- Array of host objects
    hosts:
      - name: alertmanager.cloudydad.world
        path:
          - /
        port: web

    # -- Array of TLS objects
    tls: []
    #   - secretName: alertmanager-ingress-tls
    #     hosts:
    #       - alertmanager.local

    # -- Ingress controller class name
    ingressClassName: 'traefik'

    # -- Ingress path type
    pathType: Prefix

  # -- Empty dir configuration if persistence is disabled for Alertmanager
  emptyDir: {}
  persistentVolume:
    # -- Create/use Persistent Volume Claim for alertmanager component. Empty dir if false
    enabled: false
    # -- Array of access modes. Must match those of existing PV or dynamic provisioner. Details are [here](http://kubernetes.io/docs/user-guide/persistent-volumes/)
    accessModes:
      - ReadWriteOnce
    # -- Persistant volume annotations
    annotations: {}
    # -- StorageClass to use for persistent volume. Requires alertmanager.persistentVolume.enabled: true. If defined, PVC created automatically
    storageClassName: ''
    # -- Existing Claim name. If defined, PVC must be created manually before volume will be bound
    existingClaim: ''
    # -- Mount path. Alertmanager data Persistent Volume mount root path.
    mountPath: /data
    # -- Mount subpath
    subPath: ''
    # -- Size of the volume. Better to set the same as resource limit memory property.
    size: 50Mi

  # -- Additional hostPath mounts
  extraHostPathMounts:
    []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   subPath: ""
    #   hostPath: /etc/kubernetes/certs
  #   readOnly: true

  # -- Extra Volumes for the pod
  extraVolumes:
    []
    #- name: example
    #  configMap:
    #    name: example

  # -- Extra Volume Mounts for the container
  extraVolumeMounts:
    []
    # - name: example
    #   mountPath: /example

  # -- Extra containers to run in a pod with alertmanager
  extraContainers:
    []
    #- name: config-reloader
    #  image: reloader-image

  # -- Additional initContainers to initialize the pod
  initContainers: []

# -- Add extra specs dynamically to this chart
extraObjects: []

# -- Enterprise license key configuration for VictoriaMetrics enterprise.
# Required only for VictoriaMetrics enterprise. Check docs [here](https://docs.victoriametrics.com/enterprise),
# for more information, visit [site](https://victoriametrics.com/products/enterprise/).
# Request a trial license [here](https://victoriametrics.com/products/enterprise/trial/)
# Supported starting from VictoriaMetrics v1.94.0
license:
  # -- License key
  key: ''

  # -- Use existing secret with license key
  secret:
    # -- Existing secret name
    name: ''
    # -- Key in secret with license key
    key: ''
